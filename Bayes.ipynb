{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Theorem \n",
    "\n",
    "Before introducing Parametric inference with MLE, MAP and Bayes' theorem, it is necessary to understand Bayes’ theorem (Bayes' Law) and its different components. What makes Bayes' theorem useful is that it allows incorporation of some knowledge or belief that we already have (commonly known as the prior) to help us calculate the probability of a related event. Looking back at our initial example, we identified the need for having some ability to incorporate our prior beliefs or understanding in order to figure out if our mood has any impact on the weather. \n",
    "\n",
    "This lesson will explain how we can consider doing this. \n",
    "\n",
    "## Objectives\n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical definition\n",
    "Bayes theorem is a special case of conditional probabilities and mathematically this theorem is defined as:\n",
    "\n",
    "<img src=\"bayes1.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you would derive this equation from what we know already from previous lessons:\n",
    "<img src=\"bayes3.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here A and B are random events, $P(A|B)$ is the conditional probability that event A occurs given that event B has already occurred. $P(B|A)$ has the same meaning but with the roles of A and B reversed.  P(A) and P(B) are the called marginal probabilities of event A and event B occurring respectively.\n",
    "\n",
    "> **Marginal Probability**: Probability of any single event occurring unconditioned on any other events. Whenever someone asks you whether the weather is going to be rainy or sunny today, you are computing a marginal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes’ Theorem Example #1\n",
    "\n",
    "In a medical context, we might be interested in finding out a patient’s probability of having liver disease if they are an alcoholic. “Being an alcoholic” is the key test for liver disease.\n",
    "\n",
    "* A could be the event “Patient has liver disease.” Past data tells us that 10% of patients entering your clinic have liver disease. \n",
    "\n",
    "$$P(A) = 0.10$$\n",
    "\n",
    "* B could be the event that “Patient is an alcoholic.” Five percent of the clinic’s patients are alcoholics. \n",
    "\n",
    "$$P(B) = 0.05$$\n",
    "\n",
    "We also know that among those patients diagnosed with liver disease, 7% are alcoholics. This is our P(B|A) i.e. the probability that a patient is alcoholic, **given** that they have liver disease.\n",
    "\n",
    "### Bring in the Bayes’ theorem \n",
    "\n",
    "$$P(A|B) = (0.07 * 0.1)/0.05 = 0.14$$\n",
    "\n",
    "In other words, if the patient is an alcoholic, their chances of having liver disease is 0.14 (14%). This is a large increase from the 10% suggested by past data. But it is still unlikely that any particular patient has liver disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PRIOR Probability\n",
    "\n",
    "This is calculated as $P(A)$ on the right hand side is the expression that is known as the **prior**. $P(A)$ is the marginal probability of patient having liver disease, regardless of their alcohol consumption. i.e. no conditioning taking place here. This is called the \"prior\" as it reflects the prior probability of the disease. \n",
    "\n",
    "So we effectively have this prior knowledge before deciding anything on the effect of alcohol on liver disease. This is a fundamental step that allows incorporation of what we already know before we solve a new problem.  \n",
    "\n",
    "In some cases we may not have any prior information in terms of frequencies to calculate the marginal prior probability. In such cases we can create a subjective prior where using our experience of selling ice cream (or some domain knowledge to make informed guesses).So it is possible to have a P(A) probability that is not based on any real data. As we shall see later, Bayesian inference allows us to improve our priors if they do not match with incoming data. \n",
    "\n",
    "## The LIKELIHOOD\n",
    "\n",
    "$P(B|A)$ in our bayesian equation is the likelihood. it shows the probability of a patient being alcoholic , GIVEN that he has a liver disease. It’s not a probability, but it is **proportional** to a probability. The likelihood of a hypothesis (B) given some data (A) is proportional to the probability of obtaining B given that A is true. Since a likelihood isn’t actually a probability it doesn’t obey various rules of probability. For example, likelihood need not sum to 1.\n",
    "\n",
    "A critical difference between probability and likelihood is in the interpretation of what is fixed and what can vary. In the case of a conditional probability, P(A|B), the hypothesis is fixed and the data are free to vary. Likelihood, however, is the opposite. The likelihood of a hypothesis, $L(Hypothesis|Data)$, conditions on the data as if they are fixed while allowing the hypotheses to vary.\n",
    "\n",
    "The distinction is hard to grasp at first,.\n",
    "\n",
    ">For conditional probability, the hypothesis is treated as a given and the data are free to vary. For likelihood, the data are a given and the hypotheses vary.\n",
    "\n",
    "## The Posterior Probability \n",
    "\n",
    "$P(A|B)$ is known as the posterior probability and is the revised probability of an event occurring after taking into consideration new information. Posterior probability is calculated by updating the prior probability by using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account. For our example above, this reflects th probability of having a liver disease, given that the patient is an alcoholic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem with Sensitivity and Specificity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem elegantly demonstrates the effect of false positives and false negatives in medical tests.\n",
    "\n",
    "> **Sensitivity** is the true positive rate. It is a measure of the proportion of correctly identified positives. \n",
    "\n",
    "For example, in a pregnancy test, it would be the percentage of women with a positive pregnancy test who were pregnant. A sensitive test rarely misses a \"positive.\"\n",
    "\n",
    "> **Specificity** is the true negative rate. It measures the proportion of correctly identified negatives. \n",
    "\n",
    "For example, in a pregnancy test, it would be the percent of women with a negative pregnancy test who were not pregnant. A specific test rarely registers a false positive.\n",
    "\n",
    "**A perfect test would be 100 percent sensitive and specific**. In reality, tests have a minimum error called the Bayes error rate.\n",
    "\n",
    "### Example \n",
    "\n",
    "consider a drug test that is 99 percent sensitive and 99 percent specific. If half a percent (0.5 percent) of people use a drug, what is the probability a random person with a positive test actually is a user?\n",
    "\n",
    "$$P(A ∣ B) = P(B ∣ A)P(A) / P(B)$$\n",
    "\n",
    "maybe rewritten as:\n",
    "\n",
    "$$P(user ∣ +) = P(+ ∣ user)P(user) / P(+)$$\n",
    "\n",
    "$$P(user ∣ +) = P(+ ∣ user)P(user) / [P(+ ∣ user)P(user) + P(+ ∣ non-user)P(non-user)] $$\n",
    "\n",
    "$$P(user ∣ +) = (0.99 * 0.005) / (0.99 * 0.005+0.01 * 0.995)$$\n",
    "\n",
    "$$P(user ∣ +) ≈ 33.2%$$\n",
    "\n",
    "Only about 33 percent of the time would a random person with a positive test actually be a drug user. The conclusion is that even if a person tests positive for a drug, it is more likely they do not use the drug than that they do. In other words, the number of false positives is greater than the number of true positives.\n",
    "\n",
    "In real-world situations, a trade-off is usually made between sensitivity and specificity, depending on whether it's more important to not miss a positive result or whether it's better to not label a negative result as a positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: In above example we have used baysian theorem in terms of odds/ratios, hence a slightly more complex denominator. [Visit this link](https://stats.stackexchange.com/questions/250522/difference-between-conditional-probability-and-bayes-rule) to get an insight into this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bayesian Inference is a process of deducing properties about a probability distribution from data using Bayes’ theorem.\n",
    "\n",
    "SO now what a prior is all about, let's resort back to our coin tossing experiments to understand the idea behind Bayesian inference.\n",
    "\n",
    "### An important part of Bayesian inference is the establishment of parameters and models.\n",
    "\n",
    "Analytical models are the mathematical formulation of the observed events and model parameters are the **factors** in the models affecting the observed data. Here we shall look at the factors of a Bayesian model.\n",
    "\n",
    "While tossing a coin, fairness of coin may be defined as the parameter of coin denoted by θ. The outcome of the events may be denoted by D.\n",
    "\n",
    "Think of this question. **What would be the probability of 4 heads out of 9 tosses(event D) given the fairness of coin (θ). i.e $P(D|θ)$**. \n",
    "Is this a fair question ?\n",
    "\n",
    "The answer is \"NO\". We should be more interested in knowing the **probbaility of coin being fair (θ=0.5), that given an outcome (D).** Lets represent it using Bayes Theorem:\n",
    "\n",
    "$$P(θ|D)=\\frac{P(D|θ) . P(θ)}{P(D)}$$\n",
    "\n",
    "> **P(θ) is the PRIOR** \n",
    "\n",
    "Prior reflects the **strength** of our belief in the fairness of coin before the toss. It is perfectly okay to believe that coin can have any degree of fairness between 0 and 1.\n",
    "\n",
    "> **P(D|θ) is known as the the LIKELIHOOD of observing our result given our distribution for θ. **\n",
    "\n",
    "If we knew that coin was fair, this gives the probability of observing the number of heads in a particular number of flips.\n",
    "\n",
    "> **P(D) is the EVIDENCE.**\n",
    "\n",
    "This is the probability of data as determined by summing (or integrating) across all possible values of θ, weighted by how strongly we believe in those particular values of θ. If we had multiple views of what the fairness of the coin is, the evidence tells us the probability of seeing a certain sequence of flips for all possibilities of our belief in the coin’s fairness.\n",
    "\n",
    "> **P(θ|D) is the POSTERIOR belief of our parameters after observing the evidence i.e the number of heads .**\n",
    "\n",
    "To define our Bayesian model correctly , we need two models to start with. One to represent the likelihood function $P(D|θ)$,  and the other for representing the distribution of prior beliefs . The product of these two gives the posterior belief $P(θ|D)$ distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.probabilisticworld.com/anatomy-bayes-theorem/\n",
    "[visual examples]https://www.bayestheorem.net/    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
